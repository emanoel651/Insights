# üîß Imports e Configura√ß√µes
import numpy as np
import streamlit as st
import pandas as pd
import plotly.express as px
from pathlib import Path

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# ----------------------------------------------------------------------
# P√°gina e t√≠tulo
# ----------------------------------------------------------------------
st.set_page_config(page_title="Insights", layout="wide")
st.title("üí° Insights Gerais ‚Äî Plenum + Instituto")

# ----------------------------------------------------------------------
# Fun√ß√£o para carregar dados com cache
# ----------------------------------------------------------------------
@st.cache_data
def load_data(filename: str) -> pd.DataFrame:
    try:
        base = Path(__file__).resolve().parent.parent
    except NameError:
        base = Path.cwd()
    path = base / filename
    df = pd.read_excel(path, engine="openpyxl")
    df.columns = df.columns.str.strip()

    valor_cols = [c for c in df.columns if "Valor" in c]
    if valor_cols:
        df = df.rename(columns={valor_cols[0]: "Valor_Servicos"})

    df["Emiss√£o"] = pd.to_datetime(df["Emiss√£o"], errors="coerce")

    if df["Valor_Servicos"].dtype == object:
        df["Valor_Servicos"] = (
            df["Valor_Servicos"]
              .str.replace(r"\.", "", regex=True)
              .str.replace(",", ".", regex=False)
        )
    df["Valor_Servicos"] = pd.to_numeric(df["Valor_Servicos"], errors="coerce")
    df = df.dropna(subset=["Emiss√£o", "Valor_Servicos", "Mesorregiao", "Microrregiao", "Cidade"])
    return df

# ----------------------------------------------------------------------
# Carregamento e jun√ß√£o dos dados
# ----------------------------------------------------------------------
_df_inst  = load_data("Institulo_2024-2025_ordenado.xlsx")
df_plenum = load_data("Plenum_2024-2025_ordenado.xlsx")
df_all    = pd.concat([_df_inst, df_plenum], ignore_index=True)

# ----------------------------------------------------------------------
# M√©tricas de topo
# ----------------------------------------------------------------------
st.metric("üí∞ Total de Vendas", f"R$ {df_all['Valor_Servicos'].sum():,.2f}")

# Evolu√ß√£o mensal
df_time = df_all.groupby(df_all["Emiss√£o"].dt.to_period("M"))["Valor_Servicos"].sum().reset_index()
df_time["Emiss√£o"] = df_time["Emiss√£o"].dt.strftime("%Y-%m")
st.plotly_chart(px.line(df_time, x="Emiss√£o", y="Valor_Servicos",
                        title="Evolu√ß√£o Mensal de Vendas",
                        labels={"Emiss√£o":"M√™s", "Valor_Servicos":"Vendas (R$)"}),
                use_container_width=True)

# ----------------------------------------------------------------------
# Fun√ß√£o auxiliar para top N
# ----------------------------------------------------------------------
def top_n(df, by, n=10):
    return df.groupby(by)["Valor_Servicos"].sum().reset_index().sort_values("Valor_Servicos", ascending=False).head(n)

# Top Mesorregi√µes
st.subheader("Top 10 Mesorregi√µes")
st.plotly_chart(px.bar(top_n(df_all, "Mesorregiao"), x="Valor_Servicos", y="Mesorregiao", orientation="h",
                       title="Top Mesorregi√µes", labels={"Valor_Servicos":"R$", "Mesorregiao":"Mesorregi√£o"}),
                use_container_width=True)

# Top Microrregi√µes
st.subheader("Top 10 Microrregi√µes")
st.plotly_chart(px.bar(top_n(df_all, "Microrregiao"), x="Valor_Servicos", y="Microrregiao", orientation="h",
                       title="Top Microrregi√µes", labels={"Valor_Servicos":"R$", "Microrregiao":"Microrregi√£o"}),
                use_container_width=True)

# Top Cidades
st.subheader("Top 10 Cidades")
st.plotly_chart(px.bar(top_n(df_all, "Cidade"), x="Valor_Servicos", y="Cidade", orientation="h",
                       title="Top Cidades", labels={"Valor_Servicos":"R$", "Cidade":"Cidade"}),
                use_container_width=True)

# ----------------------------------------------------------------------
# Agrupamento de dados para an√°lise
# ----------------------------------------------------------------------
df_summary = df_all.groupby(["Mesorregiao","Microrregiao"])["Valor_Servicos"].agg(
    Valor_Servicos_Total="sum",
    Valor_Servicos_Medio="mean",
    Numero_Servicos="count"
).reset_index()

# ----------------------------------------------------------------------
# Clustering
# ----------------------------------------------------------------------
st.subheader("Clustering de Regi√µes")
le_meso  = LabelEncoder().fit(df_summary["Mesorregiao"])
le_micro = LabelEncoder().fit(df_summary["Microrregiao"])
df_summary["meso_enc"]  = le_meso.transform(df_summary["Mesorregiao"])
df_summary["micro_enc"] = le_micro.transform(df_summary["Microrregiao"])

X_cluster = df_summary[["Valor_Servicos_Total", "Valor_Servicos_Medio", "Numero_Servicos", "meso_enc", "micro_enc"]]
X_scaled = StandardScaler().fit_transform(X_cluster)

# K-Means
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X_scaled)
df_summary["cluster_km"] = kmeans.labels_
st.metric("Silhouette K-Means", f"{silhouette_score(X_scaled, kmeans.labels_):.3f}")

pca_coords = PCA(n_components=2).fit_transform(X_scaled)
st.plotly_chart(px.scatter(x=pca_coords[:,0], y=pca_coords[:,1], color=df_summary["cluster_km"].astype(str),
                           title="Clusters K‚ÄëMeans (PCA)", labels={"x":"PC1","y":"PC2"},
                           hover_data={"Mesorregiao":df_summary["Mesorregiao"],
                                       "Microrregiao":df_summary["Microrregiao"]}),
                use_container_width=True)

# DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
labels_db = dbscan.labels_
n_clusters_db = len(set(labels_db)) - (1 if -1 in labels_db else 0)
if n_clusters_db >= 2:
    sil_db = silhouette_score(X_scaled[labels_db != -1], labels_db[labels_db != -1])
    st.metric("Silhouette DBSCAN", f"{sil_db:.3f}")
else:
    st.warning("DBSCAN n√£o gerou clusters suficientes para silhouette.")
st.dataframe(pd.Series(labels_db).value_counts().rename_axis("Cluster").reset_index(name="Count"))

# ----------------------------------------------------------------------
# Random Forest: Import√¢ncia das vari√°veis
# ----------------------------------------------------------------------
st.subheader("Import√¢ncia das Features")
X_rf = df_summary[["Valor_Servicos_Medio", "Numero_Servicos", "meso_enc", "micro_enc"]]
y_rf = df_summary["Valor_Servicos_Total"]
feat_imp = pd.Series(RandomForestRegressor(n_estimators=100, random_state=42).fit(X_rf, y_rf).feature_importances_, index=X_rf.columns).sort_values()
st.plotly_chart(px.bar(feat_imp, x=feat_imp.values, y=feat_imp.index, orientation="h",
                       title="Import√¢ncia das Features", labels={"x":"Import√¢ncia", "y":"Feature"}),
                use_container_width=True)

# ----------------------------------------------------------------------
# Classifica√ß√£o: ‚ÄúVale Investir‚Äù
# ----------------------------------------------------------------------
st.subheader("Classifica√ß√£o: ‚ÄúVale Investir‚Äù")

# Limiar (70¬∫ percentil)
threshold = df_summary["Valor_Servicos_Total"].quantile(0.7)
df_summary["vale_investir"] = (df_summary["Valor_Servicos_Total"] >= threshold).astype(int)

# Modelo de regress√£o log√≠stica
X = df_summary[["Valor_Servicos_Medio", "Numero_Servicos"]]
y = df_summary["vale_investir"]
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)

clf = LogisticRegression(random_state=42).fit(X_train, y_train)
df_summary["proba_v_investir"] = clf.predict_proba(X)[:, 1]

# Gr√°fico: Mesorregi√µes com alta probabilidade
st.subheader("üìà Mesorregi√µes que Valem Investir")
df_meso_inv = (
    df_summary[df_summary["vale_investir"] == 1]
      .groupby("Mesorregiao")["proba_v_investir"]
      .mean()
      .reset_index()
      .sort_values("proba_v_investir", ascending=False)
)
df_meso_inv["prob_pct"] = df_meso_inv["proba_v_investir"] * 100

fig_meso_inv = px.bar(
    df_meso_inv, x="prob_pct", y="Mesorregiao", orientation="h",
    title="Probabilidade M√©dia (%) de Vale Investir ‚Äî Mesorregi√µes",
    labels={"prob_pct":"Probabilidade (%)", "Mesorregiao":"Mesorregi√£o"},
    text="prob_pct"
)
fig_meso_inv.update_traces(texttemplate="%{text:.1f}%")
st.plotly_chart(fig_meso_inv, use_container_width=True)

# Gr√°fico: Microrregi√µes que valem investir
st.subheader("üìä Microrregi√µes que Valem Investir")
df_micro_inv = (
    df_summary[df_summary["vale_investir"] == 1]
      .loc[:, ["Microrregiao", "proba_v_investir"]]
      .sort_values("proba_v_investir", ascending=False)
      .reset_index(drop=True)
)
df_micro_inv["prob_pct"] = df_micro_inv["proba_v_investir"] * 100
fig_micro_inv = px.bar(
    df_micro_inv, x="prob_pct", y="Microrregiao", orientation="h",
    title="Probabilidade M√©dia (%) de Vale Investir ‚Äî Microrregi√µes",
    labels={"prob_pct":"Probabilidade (%)", "Microrregiao":"Microrregi√£o"},
    text="prob_pct"
)
fig_micro_inv.update_traces(texttemplate="%{text:.1f}%")
st.plotly_chart(fig_micro_inv, use_container_width=True)

# Avalia√ß√£o do modelo
st.subheader("Avalia√ß√£o do Modelo ‚ÄúVale Investir‚Äù")
st.write(f"Threshold usado: R$ {threshold:,.2f}")

y_pred = clf.predict(X_test)
st.write(f"Acur√°cia: {accuracy_score(y_test, y_pred):.3f}")

st.markdown("**Relat√≥rio de Classifica√ß√£o:**")
st.text(classification_report(y_test, y_pred, target_names=["N√£o Vale", "Vale"]))

# Matriz de confus√£o
cm = confusion_matrix(y_test, y_pred)
cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)
labels_cm = np.array([[f"{cm[i,j]}\n{cm_norm[i,j]*100:.1f}%" for j in range(cm.shape[1])] for i in range(cm.shape[0])])
fig_cm, ax = plt.subplots(figsize=(6,5))
sns.heatmap(cm_norm, annot=labels_cm, fmt="", cmap="Blues", cbar=False, linewidths=0.5, annot_kws={"size":14}, ax=ax)
ax.set_title("Matriz de Confus√£o (com porcentagens)", fontsize=16)
ax.set_xlabel("Classe Predita", fontsize=14)
ax.set_ylabel("Classe Verdadeira", fontsize=14)
ax.set_xticklabels(["N√£o Vale","Vale"], rotation=0)
ax.set_yticklabels(["N√£o Vale","Vale"], rotation=0)
plt.tight_layout()
st.pyplot(fig_cm)

# ----------------------------------------------------------------------
# Crescimento percentual geral e por regi√£o
# ----------------------------------------------------------------------
st.subheader("üìà Crescimento nas Vendas")

first_val = df_time["Valor_Servicos"].iloc[0]
last_val  = df_time["Valor_Servicos"].iloc[-1]
growth = (last_val - first_val) / first_val * 100 if first_val != 0 else 0

# Mesorregi√£o
meso_period = df_all.groupby(["Mesorregiao", df_all["Emiss√£o"].dt.to_period("M")])["Valor_Servicos"].sum().reset_index()
growths_meso = []
for meso in meso_period["Mesorregiao"].unique():
    ts = meso_period[meso_period["Mesorregiao"] == meso].sort_values("Emiss√£o")["Valor_Servicos"]
    if len(ts) > 1 and ts.iloc[0] != 0:
        growths_meso.append((ts.iloc[-1] - ts.iloc[0]) / ts.iloc[0] * 100)
avg_growth_meso = np.mean(growths_meso) if growths_meso else 0

# Microrregi√£o
micro_period = df_all.groupby(["Microrregiao", df_all["Emiss√£o"].dt.to_period("M")])["Valor_Servicos"].sum().reset_index()
growths_micro = []
for micro in micro_period["Microrregiao"].unique():
    ts = micro_period[micro_period["Microrregiao"] == micro].sort_values("Emiss√£o")["Valor_Servicos"]
    if len(ts) > 1 and ts.iloc[0] != 0:
        growths_micro.append((ts.iloc[-1] - ts.iloc[0]) / ts.iloc[0] * 100)
avg_growth_micro = np.mean(growths_micro) if growths_micro else 0

st.markdown(f"""
- Crescimento geral de vendas entre primeiro e √∫ltimo m√™s: **{growth:.1f}%**
- Crescimento m√©dio das mesorregi√µes: **{avg_growth_meso:.1f}%**
- Crescimento m√©dio das microrregi√µes: **{avg_growth_micro:.1f}%**
""")

# ----------------------------------------------------------------------
# Conclus√£o
# ----------------------------------------------------------------------
# Dados para formata√ß√£o
total_vendas = df_all["Valor_Servicos"].sum()
v_jan = int(df_time["Valor_Servicos"].iloc[0] / 1000)
v_fev = int(df_time["Valor_Servicos"].iloc[1] / 1000)
v_pico = int(df_time["Valor_Servicos"].max() / 1000)
v_abr = int(df_time["Valor_Servicos"].iloc[-2] / 1000)
v_mai = int(df_time["Valor_Servicos"].iloc[-1] / 1000)
acuracia = accuracy_score(y_test, y_pred) * 100
recall_vale = classification_report(y_test, y_pred, output_dict=True)["Vale"]["recall"]
threshold_fmt = threshold
crescimento = growth
cres_meso = avg_growth_meso
cres_micro = avg_growth_micro

# HTML como string separada
html = f"""
<div style="font-size:1.3rem; line-height:1.5;">
  <h2 style="font-size:2rem; margin-bottom:0.5rem;">üìù Conclus√µes e Pontos Positivos</h2>

  <p><strong>Panorama geral de vendas</strong> ‚Äì O total de vendas combinando Plenum e Instituto √© de R$¬†{total_vendas:,.2f}. A evolu√ß√£o mensal mostra forte oscila√ß√£o em 2024: o ano inicia com cerca de R$‚ÄØ{v_jan}‚ÄØmil em janeiro, sobe para ~R$‚ÄØ{v_fev}‚ÄØmil em fevereiro, recua em mar√ßo e estabiliza entre R$‚ÄØ200‚ÄØmil e R$‚ÄØ400‚ÄØmil at√© o fim do ano. Em 2025 surge um pico expressivo em mar√ßo (‚âàR$‚ÄØ{v_pico}‚ÄØmil), sinal de um evento ou campanha de vendas; em seguida h√° queda, mas as vendas de abril e maio (‚âàR$‚ÄØ{v_abr}‚ÄØmil e R$‚ÄØ{v_mai}‚ÄØmil) permanecem bem acima da m√©dia de 2024.</p>

  <p><strong>Mesorregi√µes l√≠deres de vendas</strong> ‚Äì As 10 principais mesorregi√µes apresentam forte concentra√ß√£o: Sul/Sudoeste de Minas e Metropolitana de Belo Horizonte lideram, cada uma com pouco mais de R$‚ÄØ1‚ÄØmilh√£o em vendas. Em seguida aparecem Zona da Mata (~R$‚ÄØ800‚ÄØmil) e Norte de Minas (~R$‚ÄØ700‚ÄØmil), enquanto regi√µes como Araraquara e Jequitinhonha ficam abaixo de R$‚ÄØ300‚ÄØmil. Isso indica que o mercado est√° muito mais aquecido no sudoeste e na capital mineira.</p>

  <p><strong>Microrregi√µes mais lucrativas</strong> ‚Äì O ranking por microrregi√£o mostra uma distribui√ß√£o mais equilibrada: Itabira lidera com ~R$‚ÄØ300‚ÄØmil, seguida de Manhua√ßu (~R$‚ÄØ270‚ÄØmil), Pouso Alegre e Belo Horizonte (~R$‚ÄØ250‚ÄØmil cada). Mesmo as microrregi√µes menores, como Varginha e Patroc√≠nio, mant√™m vendas superiores a R$‚ÄØ100‚ÄØmil. Isso sugere que diversificar a atua√ß√£o em diferentes microrregi√µes pode trazer bons resultados.</p>

  <p><strong>Maiores cidades vendedoras</strong> ‚Äì No n√≠vel municipal, Mat√£o √© o grande destaque com ~R$‚ÄØ200‚ÄØmil em vendas. Manhua√ßu (~R$‚ÄØ187‚ÄØmil), Belo Vale (~R$‚ÄØ160‚ÄØmil), Ub√° (~R$‚ÄØ150‚ÄØmil) e Nepomuceno (~R$‚ÄØ140‚ÄØmil) tamb√©m se destacam. A diferen√ßa relativamente pequena entre as cidades evidencia que nenhuma √∫nica cidade domina o mercado; o portf√≥lio de vendas √© mais distribu√≠do.</p>

  <p><strong>Probabilidade de ‚ÄúVale Investir‚Äù por regi√£o</strong> ‚Äì O modelo de recomenda√ß√£o calcula a probabilidade de uma regi√£o valer a pena para investimento. Entre as mesorregi√µes, Araraquara (100‚ÄØ%), Vale do Mucuri (~99‚ÄØ%) e Tri√¢ngulo Mineiro/Alto Parana√≠ba (~98‚ÄØ%) s√£o as mais promissoras; j√° Centro Norte Baiano tem apenas 23‚ÄØ%, indicando maior risco. No n√≠vel de microrregi√£o, a maior parte apresenta probabilidade muito alta (pr√≥xima a 100‚ÄØ%) ‚Äì destaque para Itabira, Cataguases, Lavras e Juiz de Fora. Microrregi√µes como Vit√≥ria, Almenara e Pirassununga ficam abaixo de 40‚ÄØ%, sugerindo cautela.</p>

  <p><strong>Desempenho do modelo de classifica√ß√£o</strong> ‚Äì A classifica√ß√£o bin√°ria (‚ÄúVale investir‚Äù vs. ‚ÄúN√£o vale‚Äù) usa como limiar o 70.¬∫ percentil de vendas (R$‚ÄØ{threshold_fmt:,.2f}). O modelo atingiu {acuracia:.1f}‚ÄØ% de acur√°cia; ele identifica corretamente todos os casos de ‚ÄúN√£o Vale‚Äù (recall 1,00), e acerta todas as vezes que classifica algo como ‚ÄúVale‚Äù (precis√£o 1,00). O recall para ‚ÄúVale‚Äù √© {recall_vale:.2f}, indicando que ainda deixa de marcar alguns investimentos potencialmente bons.</p>

  <p><strong>Resumo:</strong></p>
  <ul>
    <li>üìà Crescimento geral nas vendas: <strong>{crescimento:.1f}%</strong></li>
    <li>üìä Crescimento m√©dio nas mesorregi√µes: <strong>{cres_meso:.1f}%</strong></li>
    <li>üìç Crescimento m√©dio nas microrregi√µes: <strong>{cres_micro:.1f}%</strong></li>
  </ul>

  <p><strong>An√°lise final</strong>: A plataforma revela um crescimento s√≥lido em 2025, apoiado por um pico de vendas em mar√ßo. O mercado √© fortemente concentrado em poucas mesorregi√µes (especialmente Sul/Sudoeste de Minas e a regi√£o metropolitana de BH), mas v√°rias microrregi√µes e cidades menores contribuem significativamente para o faturamento. As probabilidades de ‚Äúvale investir‚Äù indicam que, al√©m do volume de vendas, algumas regi√µes possuem alto potencial de retorno ‚Äì em especial Araraquara e Itabira. O modelo de classifica√ß√£o √© confi√°vel (alto precision e recall), embora ainda possa melhorar a sensibilidade para identificar todas as regi√µes de alto potencial.</p>
</div>
"""

# Renderiza
st.markdown(html, unsafe_allow_html=True)


