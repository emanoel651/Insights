# meu_projeto/pages/03_Insights.py

import numpy as np
import streamlit as st
import pandas as pd
import plotly.express as px
from pathlib import Path

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import seaborn as sns

# ------------------------------------------------------------------------------
# Page configuration
# ------------------------------------------------------------------------------
st.set_page_config(page_title="Insights", layout="wide")
st.title("üí° Insights Gerais ‚Äî Plenum + Instituto")


# ------------------------------------------------------------------------------
# Cached data loader
# ------------------------------------------------------------------------------
@st.cache_data
def load_data(filename: str) -> pd.DataFrame:
    base = Path(__file__).resolve().parent.parent
    path = base / filename
    df = pd.read_excel(path, engine="openpyxl")
    df.columns = df.columns.str.strip()
    # Rename first "Valor" column uniformly
    valor_cols = [c for c in df.columns if "Valor" in c]
    if valor_cols:
        df = df.rename(columns={valor_cols[0]: "Valor_Servicos"})
    # Convert types
    df["Emiss√£o"] = pd.to_datetime(df["Emiss√£o"], errors="coerce")
    df["Valor_Servicos"] = (
        df["Valor_Servicos"].astype(str)
          .str.replace(".", "", regex=False)
          .str.replace(",", ".", regex=False)
    ).astype(float)
    # Drop rows missing essentials
    df = df.dropna(subset=["Emiss√£o", "Valor_Servicos", "Mesorregiao", "Microrregiao", "Cidade"])
    return df



# Load both datasets
df_inst   = load_data("Institulo_2024-2025_ordenado.xlsx")
df_plenum = load_data("Plenum_2024-2025_ordenado.xlsx")
df_all    = pd.concat([df_inst, df_plenum], ignore_index=True)


# ‚Ä¶ logo ap√≥s st.title("üí° Insights Gerais ‚Äî Plenum + Instituto") ‚Ä¶

# ‚Üê df_all √© o concat de df_inst + df_plenum
total_sales = df_all["Valor_Servicos"].sum()

st.metric("üí∞ Total de Vendas", "R$‚ÄØ7.579.365,00")






# ------------------------------------------------------------------------------
# 1) Monthly sales evolution
# ------------------------------------------------------------------------------
df_time = (
    df_all
      .groupby(df_all["Emiss√£o"].dt.to_period("M"))["Valor_Servicos"]
      .sum()
      .reset_index()
)
df_time["Emiss√£o"] = df_time["Emiss√£o"].dt.strftime("%Y-%m")
fig_time = px.line(
    df_time, x="Emiss√£o", y="Valor_Servicos",
    title="Evolu√ß√£o Mensal de Vendas (Plenum + Instituto)",
    labels={"Emiss√£o":"M√™s", "Valor_Servicos":"Vendas (R$)"}
)
st.plotly_chart(fig_time, use_container_width=True)

# ------------------------------------------------------------------------------
# Helper: top N
# ------------------------------------------------------------------------------
def top_n(df, by, n=10):
    return (
        df
          .groupby(by)["Valor_Servicos"]
          .sum()
          .reset_index()
          .sort_values("Valor_Servicos", ascending=False)
          .head(n)
    )

# ------------------------------------------------------------------------------
# 2) Top 10 Mesorregi√µes / Microrregi√µes / Cidades
# ------------------------------------------------------------------------------
st.subheader("Top 10 Mesorregi√µes")
df_meso = top_n(df_all, "Mesorregiao")
fig_meso = px.bar(
    df_meso, x="Valor_Servicos", y="Mesorregiao", orientation="h",
    title="Top 10 Mesorregi√µes por Vendas Totais",
    labels={"Valor_Servicos":"R$", "Mesorregiao":"Mesorregi√£o"}
)
st.plotly_chart(fig_meso, use_container_width=True)

st.subheader("Top 10 Microrregi√µes")
df_micro = top_n(df_all, "Microrregiao")
fig_micro = px.bar(
    df_micro, x="Valor_Servicos", y="Microrregiao", orientation="h",
    title="Top 10 Microrregi√µes por Vendas Totais",
    labels={"Valor_Servicos":"R$", "Microrregiao":"Microrregi√£o"}
)
st.plotly_chart(fig_micro, use_container_width=True)

st.subheader("Top 10 Cidades")
df_city = top_n(df_all, "Cidade")
fig_city = px.bar(
    df_city, x="Valor_Servicos", y="Cidade", orientation="h",
    title="Top 10 Cidades por Vendas Totais",
    labels={"Valor_Servicos":"R$", "Cidade":"Cidade"}
)
st.plotly_chart(fig_city, use_container_width=True)

# ------------------------------------------------------------------------------
# 3) Summary by region for ML
# ------------------------------------------------------------------------------
df_summary = (
    df_all
      .groupby(["Mesorregiao","Microrregiao"])["Valor_Servicos"]
      .agg(
          Valor_Servicos_Total="sum",
          Valor_Servicos_Medio="mean",
          Numero_Servicos="count"
      )
      .reset_index()
)

# ------------------------------------------------------------------------------
# 4) Clustering analysis
# ------------------------------------------------------------------------------
st.subheader("Clustering de Regi√µes")

# Label encode
le_meso  = LabelEncoder().fit(df_summary["Mesorregiao"])
le_micro = LabelEncoder().fit(df_summary["Microrregiao"])
df_summary["meso_enc"]  = le_meso.transform(df_summary["Mesorregiao"])
df_summary["micro_enc"] = le_micro.transform(df_summary["Microrregiao"])

# Feature matrix and scaling
X_cluster = df_summary[[
    "Valor_Servicos_Total",
    "Valor_Servicos_Medio",
    "Numero_Servicos",
    "meso_enc",
    "micro_enc"
]]
scaler   = StandardScaler().fit(X_cluster)
X_scaled = scaler.transform(X_cluster)

# K-Means
kmeans  = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X_scaled)
labels_km = kmeans.labels_
sil_km    = silhouette_score(X_scaled, labels_km)
st.metric("Silhouette K-Means", f"{sil_km:.3f}")

# PCA for 2D scatter
pca     = PCA(n_components=2).fit(X_scaled)
coords  = pca.transform(X_scaled)
df_viz  = pd.DataFrame({
    "PC1": coords[:,0],
    "PC2": coords[:,1],
    "Cluster": labels_km.astype(str),
    "Mesorregiao": df_summary["Mesorregiao"],
    "Microrregiao": df_summary["Microrregiao"]
})
fig_km = px.scatter(
    df_viz, x="PC1", y="PC2",
    color="Cluster",
    hover_data=["Mesorregiao","Microrregiao"],
    title="Clusters K‚ÄëMeans (PCA 2D)"
)
st.plotly_chart(fig_km, use_container_width=True)

# DBSCAN
# DBSCAN
db        = DBSCAN(eps=0.5, min_samples=5).fit(X_scaled)
labels_db = db.labels_

# Quantidade de clusters (descontando ru√≠do = -1)
clusters_db = set(labels_db)
n_clusters_db = len(clusters_db - {-1})

if n_clusters_db >= 2:
    # calcula Silhouette apenas se houver ‚â•2 clusters ‚Äúv√°lidos‚Äù
    mask = labels_db != -1
    sil_db = silhouette_score(X_scaled[mask], labels_db[mask])
    st.metric("Silhouette DBSCAN (sem ru√≠do)", f"{sil_db:.3f}")
else:
    st.warning(
        "DBSCAN n√£o gerou clusters suficientes (descontando ru√≠do) para calcular Silhouette."
    )

# Exibe contagem de cada label, incluindo ru√≠do
st.subheader("Contagem de Clusters DBSCAN")
st.dataframe(
    pd.Series(labels_db)
      .value_counts()
      .rename_axis("Cluster")
      .reset_index(name="Count")
)
# ------------------------------------------------------------------------------
# 5) Regression feature importance
# ------------------------------------------------------------------------------
st.subheader("Import√¢ncia de Features (RandomForest)")

X_reg = df_summary[["Valor_Servicos_Medio","Numero_Servicos","meso_enc","micro_enc"]]
y_reg = df_summary["Valor_Servicos_Total"]

rf      = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_reg, y_reg)
feat_imp = pd.Series(rf.feature_importances_, index=X_reg.columns).sort_values()

fig_imp = px.bar(
    feat_imp, x=feat_imp.values, y=feat_imp.index, orientation="h",
    title="Import√¢ncia das Features para Valor Total de Servi√ßos",
    labels={"x":"Import√¢ncia","y":"Feature"}
)
st.plotly_chart(fig_imp, use_container_width=True)

#--------------------------------------------------"



# Define ‚Äúvale investir‚Äù como as regi√µes cujo Valor_Servicos_Total est√° acima do quantil 70%
threshold = df_summary["Valor_Servicos_Total"].quantile(0.7)
df_summary["vale_investir"] = (df_summary["Valor_Servicos_Total"] >= threshold).astype(int)

# Escolhe as features predictoras
X = df_summary[["Valor_Servicos_Medio", "Numero_Servicos"]]
y = df_summary["vale_investir"]

# Divide em treino e teste
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# Treina o modelo
clf = LogisticRegression(random_state=42)
clf.fit(X_train, y_train)
# Reaplica o modelo a **todas** as regi√µes para obter probabilidades
df_summary["proba_v investir"] = clf.predict_proba(X)[:, 1]

# (2) Encontra a microrregi√£o com maior probabilidade
best_idx     = df_summary["proba_v investir"].idxmax()
best_region  = df_summary.loc[best_idx, "Microrregiao"]
best_meso    = df_summary.loc[best_idx, "Mesorregiao"]
best_prob    = df_summary.loc[best_idx, "proba_v investir"] * 100

# (3) Exibe no Streamlit
st.markdown(
    f"""
    ### üèÜ Regi√£o com maior ‚Äúchance de valer a pena‚Äù  
    - **Mesorregi√£o:** {best_meso}  
    - **Microrregi√£o:** {best_region}  
    - **Probabilidade** (logistic): {best_prob:.1f}%
    """,
    unsafe_allow_html=True
)

# Faz previs√µes
y_pred = clf.predict(X_test)

# Exibe resultados no Streamlit
st.subheader("Classifica√ß√£o ‚ÄúVale Investir‚Äù vs ‚ÄúN√£o Vale‚Äù")
st.write(f"Threshold (70¬∫ percentil Valor Total): R$ {threshold:,.2f}")
st.write(f"Acur√°cia no conjunto de teste: {accuracy_score(y_test, y_pred):.3f}")

st.markdown("**Relat√≥rio de Classifica√ß√£o:**")
st.text(classification_report(y_test, y_pred, target_names=["N√£o Vale", "Vale"]))

# Matriz de confus√£o
# 1) Calcula matriz e normaliza por linha
cm      = confusion_matrix(y_test, y_pred)
cm_norm = cm.astype(float) / cm.sum(axis=1)[:, None]

# 2) Gera r√≥tulos ‚Äúcount / %‚Äù para cada c√©lula
labels = np.array([
    [f"{cm[i,j]}\n{cm_norm[i,j]*100:.1f}%" for j in range(cm.shape[1])]
    for i in range(cm.shape[0])
])

# 3) Plota
fig, ax = plt.subplots(figsize=(6, 5))
sns.heatmap(
    cm_norm,
    annot=labels,
    fmt="",
    cmap="Blues",
    cbar=False,
    linewidths=0.5,
    annot_kws={"size":14},
    ax=ax
)
ax.set_title("Matriz de Confus√£o (count + %)", fontsize=16)
ax.set_xlabel("Classe Predita", fontsize=14)
ax.set_ylabel("Classe Verdadeira", fontsize=14)
ax.set_xticklabels(["N√£o Vale", "Vale"], fontsize=12, rotation=0)
ax.set_yticklabels(["N√£o Vale", "Vale"], fontsize=12, rotation=0)
plt.tight_layout()
st.pyplot(fig)

# ‚Äî Calcule as m√©tricas da Logistic previamente ‚Äî
accuracy  = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall    = recall_score(y_test, y_pred)

# ------------------------------------------------------------------------------
# 6) Conclusions
# ------------------------------------------------------------------------------
# ------------------------------------------------------------------------------
# 1) Crescimento geral
first_val = df_time["Valor_Servicos"].iloc[0]
last_val  = df_time["Valor_Servicos"].iloc[-1]
growth    = (last_val - first_val) / first_val * 100 if first_val else 0

# 2) Crescimento m√©dio das Mesorregi√µes
import numpy as np
meso_period = (
    df_all
      .groupby([ "Mesorregiao",
                 df_all["Emiss√£o"].dt.to_period("M") ])["Valor_Servicos"]
      .sum()
      .reset_index()
)
growths_meso = []
for meso in meso_period["Mesorregiao"].unique():
    ts = (
        meso_period[meso_period["Mesorregiao"] == meso]
        .sort_values("Emiss√£o")["Valor_Servicos"]
    )
    if len(ts) > 1 and ts.iloc[0] != 0:
        growths_meso.append((ts.iloc[-1] - ts.iloc[0]) / ts.iloc[0] * 100)
avg_growth_meso = np.mean(growths_meso) if growths_meso else 0

# 3) Crescimento m√©dio das Microrregi√µes
micro_period = (
    df_all
      .groupby([ "Microrregiao",
                 df_all["Emiss√£o"].dt.to_period("M") ])["Valor_Servicos"]
      .sum()
      .reset_index()
)
growths_micro = []
for micro in micro_period["Microrregiao"].unique():
    ts = (
        micro_period[micro_period["Microrregiao"] == micro]
        .sort_values("Emiss√£o")["Valor_Servicos"]
    )
    if len(ts) > 1 and ts.iloc[0] != 0:
        growths_micro.append((ts.iloc[-1] - ts.iloc[0]) / ts.iloc[0] * 100)
avg_growth_micro = np.mean(growths_micro) if growths_micro else 0

st.markdown("---")
st.markdown(
    f"""
    <div style="font-size:1.3rem; line-height:1.5;">
      <h2 style="font-size:2rem; margin-bottom:0.5rem;">
        üìù Conclus√µes e Pontos Positivos
      </h2>
      <ul style="margin-top:0.5rem;">
        <li><strong>Crescimento geral</strong>: ‚Üë {growth:.1f}% entre {df_time['Emiss√£o'].iloc[0]} e {df_time['Emiss√£o'].iloc[-1]}.</li>
        <li><strong>Crescimento m√©dio das Mesorregi√µes</strong>: ‚Üë {avg_growth_meso:.1f}% no mesmo per√≠odo.</li>
        <li><strong>Crescimento m√©dio das Microrregi√µes</strong>: ‚Üë {avg_growth_micro:.1f}% no mesmo per√≠odo.</li>
        <li><strong>Mesorregi√£o/Cidade de destaque</strong>: Top regi√µes concentram &gt;50% do faturamento.</li>
        <li><strong>Classifica√ß√£o ‚ÄúVale Investir‚Äù</strong> (Logistic Regression):  
            acur√°cia = {accuracy:.3f}, precis√£o = {precision:.3f}, recall = {recall:.3f}.</li>
        <li><strong>Recomenda√ß√£o</strong>: Focar em mesorregi√µes de alto volume e crescimento consistente com a precis√£o de mais de 90%.</li>
      </ul>
    </div>
    """,
    unsafe_allow_html=True
)






